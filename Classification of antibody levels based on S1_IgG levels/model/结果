普通的注意力机制的结果
Evaluation - Loss: 0.2517, Accuracy: 0.7200
对类别1的Recall: 0.7391304347826086
对类别1的F1-score: 0.8095238095238095
对类别0的Recall: 0.6410256410256411
对类别0的F1-score: 0.4716981132075472
用广义加性模型代替注意力机制后的结果
Evaluation - Loss: 0.2498, Accuracy: 0.7350
对类别1的Recall: 0.7577639751552795
对类别1的F1-score: 0.8215488215488216
对类别0的Recall: 0.6410256410256411
对类别0的F1-score: 0.4854368932038835





自适应算法
Accuracy: 0.555
对类别1的Recall: 0.5652173913043478
对类别1的F1-score: 0.6715867158671587
对类别0的Recall: 0.5128205128205128
对类别0的F1-score: 0.31007751937984496



光感
0.66
对类别1的Recall: 0.6708074534161491
对类别1的F1-score: 0.7605633802816902
对类别0的Recall: 0.6153846153846154
对类别0的F1-score: 0.41379310344827586



catboost
准确率: 0.67
对类别1的Recall: 0.6832298136645962
对类别1的F1-score: 0.7692307692307692
对类别0的Recall: 0.6153846153846154
对类别0的F1-score: 0.4210526315789474


LSTM+attention
Accuracy on the test set: 63.50%
对类别1的Recall: 0.6149068322981367
对类别1的F1-score: 0.7306273062730627
对类别0的Recall: 0.717948717948718
对类别0的F1-score: 0.434108527131783

GAN
Test Accuracy: 64.50%
对类别1的Recall: 0.6211180124223602
对类别1的F1-score: 0.7380073800738007
对类别0的Recall: 0.7435897435897436
对类别0的F1-score: 0.44961240310077527

Seq2seq_NA
Accuracy: 0.7300
对类别1的Recall: 0.7701863354037267
对类别1的F1-score: 0.8211920529801325
对类别0的Recall: 0.5641025641025641
对类别0的F1-score: 0.4489795918367347